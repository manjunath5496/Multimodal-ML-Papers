<h2>Multimodal ML Papers </h2>



<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(1).pdf" style="text-decoration:none;">Attend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(2).pdf" style="text-decoration:none;">Multimodal Medical Image Retrieval based on Latent Topic Modeling</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(3).pdf" style="text-decoration:none;">Unifying and Merging Well-trained Deep Neural Networks for Inference Stage</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(4).pdf" style="text-decoration:none;">Semi-Supervised Multimodal Deep Learning for RGB-D Object Recognition</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(5).pdf" style="text-decoration:none;">Microsoft COCO: Common Objects in Context</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(6).pdf" style="text-decoration:none;">Deep Fragment Embeddings for Bidirectional Image Sentence Mapping</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(7).pdf" style="text-decoration:none;">Show and Tell: A Neural Image Caption Generator</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(8).pdf" style="text-decoration:none;"> Deep Visual-Semantic Alignments for Generating Image Descriptions </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(9).pdf" style="text-decoration:none;">A Dataset for Movie Description</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(10).pdf" style="text-decoration:none;">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(11).pdf" style="text-decoration:none;">What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(12).pdf" style="text-decoration:none;">VQA: Visual Question Answering</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(13).pdf" style="text-decoration:none;">Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(14).pdf" style="text-decoration:none;">Multimodal Deep Learning for Robust RGB-D Object Recognition</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(15).pdf" style="text-decoration:none;">Order-Embeddings of Images and Language</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(16).pdf" style="text-decoration:none;">VisualWord2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(17).pdf" style="text-decoration:none;">MovieQA: Understanding Stories in Movies through Question-Answering</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(18).pdf" style="text-decoration:none;">Hollywood in Homes: Crowdsourcing Data
Collection for Activity Understanding</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(19).pdf" style="text-decoration:none;">Generative Adversarial Text to Image Synthesis</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(20).pdf" style="text-decoration:none;">Learning to Communicate with
Deep Multi-Agent Reinforcement Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(21).pdf" style="text-decoration:none;">Review Networks for Caption Generation</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(22).pdf" style="text-decoration:none;">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(23).pdf" style="text-decoration:none;">Towards Transparent AI Systems:
Interpreting Visual Question Answering Models</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(24).pdf" style="text-decoration:none;">Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(25).pdf" style="text-decoration:none;">SoundNet: Learning Sound
Representations from Unlabeled Video</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(26).pdf" style="text-decoration:none;">Visual Dialog</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(27).pdf" style="text-decoration:none;">Multi-Agent Cooperation and the Emergence of (Natural) Language</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(28).pdf" style="text-decoration:none;">Deep Voice: Real-time Neural Text-to-Speech</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(29).pdf" style="text-decoration:none;">Zero-Shot Learning - The Good, the Bad and the Ugly </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(30).pdf" style="text-decoration:none;">Emergence of Grounded Compositional Language in Multi-Agent Populations</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(31).pdf" style="text-decoration:none;">Learning Robust Visual-Semantic Embeddings</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(32).pdf" style="text-decoration:none;">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(33).pdf" style="text-decoration:none;">Towards Building Large Scale Multimodal Domain-Aware Conversation Systems</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(34).pdf" style="text-decoration:none;">Generating Descriptions with Grounded and Co-Referenced People</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(35).pdf" style="text-decoration:none;">Deep Multimodal Representation Learning from Temporal Data</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(36).pdf" style="text-decoration:none;">Learning to Reason: End-to-End Module Networks for Visual Question Answering</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(37).pdf" style="text-decoration:none;">End-to-End Multimodal Emotion Recognition using Deep Neural Networks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(38).pdf" style="text-decoration:none;">Deep Voice 2: Multi-Speaker Neural Text-to-Speech</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(39).pdf" style="text-decoration:none;">Gated-Attention Architectures for Task-Oriented Language Grounding</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(40).pdf" style="text-decoration:none;">Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(41).pdf" style="text-decoration:none;">SCAN: Learning Hierarchical Compositional Visual Concepts</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(42).pdf" style="text-decoration:none;">Tensor Fusion Network for Multimodal Sentiment Analysis</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(43).pdf" style="text-decoration:none;">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(44).pdf" style="text-decoration:none;">Localizing Moments in Video with Natural Language</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(45).pdf" style="text-decoration:none;">Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(46).pdf" style="text-decoration:none;">Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(47).pdf" style="text-decoration:none;">Fooling Vision and Language Models
Despite Localization and Attention Mechanism</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(48).pdf" style="text-decoration:none;">Emergent Translation in Multi-Agent Communication</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(49).pdf" style="text-decoration:none;">Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(50).pdf" style="text-decoration:none;">Multimodal Probabilistic Model-Based Planning for Human-Robot Interaction</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(51).pdf" style="text-decoration:none;">Learning Multi-ModalWord Representation Grounded in Visual Context</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(52).pdf" style="text-decoration:none;">Look, Imagine and Match:
Improving Textual-Visual Cross-Modal Retrieval with Generative Models</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(53).pdf" style="text-decoration:none;">Neural Motifs: Scene Graph Parsing with Global Context</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(54).pdf" style="text-decoration:none;">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(55).pdf" style="text-decoration:none;">Video Captioning via Hierarchical Reinforcement Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(56).pdf" style="text-decoration:none;">Embodied Question Answering </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(57).pdf" style="text-decoration:none;">Don't Just Assume; Look and Answer:
Overcoming Priors for Visual Question Answering</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(58).pdf" style="text-decoration:none;">Grounding Referring Expressions in Images by Variational Context</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(59).pdf" style="text-decoration:none;">Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(60).pdf" style="text-decoration:none;">Semi-supervised Multimodal Hashing </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(61).pdf" style="text-decoration:none;"> Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(62).pdf" style="text-decoration:none;">Zero-Resource Neural Machine Translation with Multi-Agent Communication Game</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(63).pdf" style="text-decoration:none;">A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(64).pdf" style="text-decoration:none;">Multimodal Generative Models for Scalable Weakly-Supervised Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(65).pdf" style="text-decoration:none;">Learning to Count Objects in Natural Images for Visual Question Answering </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(66).pdf" style="text-decoration:none;">Multimodal Explanations: Justifying Decisions and Pointing to the Evidence</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(67).pdf" style="text-decoration:none;">Joint Event Detection and Description in Continuous Video Streams</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(68).pdf" style="text-decoration:none;">Learning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(69).pdf" style="text-decoration:none;">Look Before You Leap:
Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(70).pdf" style="text-decoration:none;">Datasheets for Datasets</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(71).pdf" style="text-decoration:none;">Neural Baby Talk</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(72).pdf" style="text-decoration:none;">Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(73).pdf" style="text-decoration:none;">Seeing Voices and Hearing Faces: Cross-modal biometric matching</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(74).pdf" style="text-decoration:none;">Jointly Discovering Visual Objects and SpokenWords from Raw Sensory Input</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(75).pdf" style="text-decoration:none;">Image Generation from Scene Graphs</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(76).pdf" style="text-decoration:none;">Multilevel Language and Vision Integration for Text-to-Clip Retrieval</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(77).pdf" style="text-decoration:none;">Learning to Color from Language</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(78).pdf" style="text-decoration:none;">Attention Based Natural Language Grounding by Navigating Virtual Environment</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(79).pdf" style="text-decoration:none;">No Metrics Are Perfect:
Adversarial Reward Learning for Visual Storytelling</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(80).pdf" style="text-decoration:none;">Multi-modal Approach for Affective Computing</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(81).pdf" style="text-decoration:none;">Charades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(82).pdf" style="text-decoration:none;">Dialog-based Interactive Image Retrieval</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(83).pdf" style="text-decoration:none;">Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(84).pdf" style="text-decoration:none;">Using Syntax to Ground Referring Expressions in Natural Images</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(85).pdf" style="text-decoration:none;">Efficient Low-rank Multimodal Fusion with Modality-Specific Factors</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(86).pdf" style="text-decoration:none;">Learning Factorized Multimodal Representations</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(87).pdf" style="text-decoration:none;">Talk the Walk: Navigating New York City through Grounded Dialogue</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(88).pdf" style="text-decoration:none;">Evolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(89).pdf" style="text-decoration:none;">Disjoint Mapping Network for Cross-modal Matching of Voices and Faces</a></li>
  
  
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(90).pdf" style="text-decoration:none;"> Multimodal Language Analysis with Recurrent Multistage Fusion</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(91).pdf" style="text-decoration:none;">Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(92).pdf" style="text-decoration:none;">RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(93).pdf" style="text-decoration:none;"> Embedding Multimodal Relational Data for Knowledge Base Completion</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(94).pdf" style="text-decoration:none;">Visual Coreference Resolution in Visual Dialog using Neural Module Networks</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(95).pdf" style="text-decoration:none;">Deep Audio-Visual Speech Recognition</a></li>  
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(96).pdf" style="text-decoration:none;">From Audio to Semantics: Approaches to end-to-end spoken language understanding</a></li> 
  
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(97).pdf" style="text-decoration:none;">Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(98).pdf" style="text-decoration:none;">MELD: A Multimodal Multi-Party Dataset
for Emotion Recognition in Conversations</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(99).pdf" style="text-decoration:none;">Overcoming Language Priors in Visual Question Answering with Adversarial Regularization</a></li>  
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(100).pdf" style="text-decoration:none;">Model Cards for Model Reporting</a></li>  
  
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(101).pdf" style="text-decoration:none;">Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(102).pdf" style="text-decoration:none;">Do Explanations make VQA Models more Predictable to a Human?</a></li> 
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(103).pdf" style="text-decoration:none;">Latent Variable Model for Multi-modal Translation </a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(104).pdf" style="text-decoration:none;">COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(105).pdf" style="text-decoration:none;">Unsupervised Multimodal Representation Learning across Medical Images and Reports</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(106).pdf" style="text-decoration:none;">EARLY FUSION for Goal Directed Robotic Vision</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(107).pdf" style="text-decoration:none;">Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation</a></li> 
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(108).pdf" style="text-decoration:none;">Show, Control and Tell:
A Framework for Generating Controllable and Grounded Captions</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(109).pdf" style="text-decoration:none;">From Recognition to Cognition: Visual Commonsense Reasoning</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(110).pdf" style="text-decoration:none;">Improving Hospital Mortality Prediction with Medical Named Entities and Multimodal Learning </a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(111).pdf" style="text-decoration:none;">TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments</a></li> 
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(112).pdf" style="text-decoration:none;">Multi-task Learning of Hierarchical Vision-Language Representation</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(113).pdf" style="text-decoration:none;">Knowledge-driven generative subspaces for modeling multi-view dependencies in medical data</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(114).pdf" style="text-decoration:none;">Multimodal Explanations by Predicting Counterfactuality in Videos</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(115).pdf" style="text-decoration:none;">Grounded Video Description</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(116).pdf" style="text-decoration:none;">Found in Translation:
Learning Robust Joint Representations by Cyclic Translations Between Modalities</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(117).pdf" style="text-decoration:none;">Self-Supervised Learning from Web Data for Multimodal Retrieval</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(118).pdf" style="text-decoration:none;">Self-Monitoring Navigation Agent via Auxiliary Progress Estimation</a></li>  
   
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(119).pdf" style="text-decoration:none;">Evaluating Text-to-Image Matching using Binary Image Selection (BISON)</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(120).pdf" style="text-decoration:none;">Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(121).pdf" style="text-decoration:none;">Embodied Multimodal Multitask Learning</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(122).pdf" style="text-decoration:none;">Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup</a></li>  
     
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(123).pdf" style="text-decoration:none;">From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(124).pdf" style="text-decoration:none;">Audio-Linguistic Embeddings for Spoken Sentences</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(125).pdf" style="text-decoration:none;">Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(126).pdf" style="text-decoration:none;">
Audio Caption: Listen and Tell</a></li> 
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(127).pdf" style="text-decoration:none;">GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(128).pdf" style="text-decoration:none;">The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(129).pdf" style="text-decoration:none;">Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(130).pdf" style="text-decoration:none;">Neural Language Modeling with Visual Features </a></li>    
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(131).pdf" style="text-decoration:none;">Learning to Speak and Act in a Fantasy Text Adventure Game</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(132).pdf" style="text-decoration:none;">On the Pitfalls of Measuring Emergent Communication</a></li>   
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(133).pdf" style="text-decoration:none;">MMKG: Multi-Modal Knowledge Graphs</a></li>     
   
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(134).pdf" style="text-decoration:none;">MFAS: Multimodal Fusion Architecture Search</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(135).pdf" style="text-decoration:none;">Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(136).pdf" style="text-decoration:none;">Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(137).pdf" style="text-decoration:none;">Habitat: A Platform for Embodied AI Research</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(138).pdf" style="text-decoration:none;">VideoBERT: A Joint Model for Video and Language Representation Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(139).pdf" style="text-decoration:none;">VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(140).pdf" style="text-decoration:none;">Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias inWord Embeddings</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(141).pdf" style="text-decoration:none;"> From Semi-supervised to Almost-unsupervised Speech Recognition with Very-low Resource by Jointly Learning Phonetic Structures from Audio and Text Embeddings</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(142).pdf" style="text-decoration:none;">Temporal Cycle-Consistency Learning</a></li>                             
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(143).pdf" style="text-decoration:none;">Emergence of Compositional Language with Deep Generational Transmission</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(144).pdf" style="text-decoration:none;">SOCIAL IQA: Commonsense Reasoning about Social Interactions</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(145).pdf" style="text-decoration:none;">The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(146).pdf" style="text-decoration:none;">Speech2Face: Learning the Face Behind a Voice</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(147).pdf" style="text-decoration:none;">Reconstructing faces from voices</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(148).pdf" style="text-decoration:none;">Leveraging Medical Visual Question Answering with Supporting Facts</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(149).pdf" style="text-decoration:none;">Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(150).pdf" style="text-decoration:none;">What Makes Training Multi-modal Classification Networks Hard?</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(151).pdf" style="text-decoration:none;">Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(152).pdf" style="text-decoration:none;">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge </a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(153).pdf" style="text-decoration:none;">Multimodal Transformer for Unaligned Multimodal Language Sequences</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(154).pdf" style="text-decoration:none;">Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(155).pdf" style="text-decoration:none;">Hierarchical Decision Making by Generating and Following Natural Language Instructions</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(156).pdf" style="text-decoration:none;">Learning Representations by Maximizing Mutual Information Across Views</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(157).pdf" style="text-decoration:none;">Exploring Phoneme-Level Speech Representations for End-to-End Speech Translation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(158).pdf" style="text-decoration:none;">Learning to Compose and Reason with
Language Tree Structures for Visual Grounding</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(159).pdf" style="text-decoration:none;">Towards Multimodal Sarcasm Detection
(An Obviously Perfect Paper) </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(160).pdf" style="text-decoration:none;">Learning Individual Styles of Conversational Gesture</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(161).pdf" style="text-decoration:none;">Lattice Transformer for Speech Translation</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(162).pdf" style="text-decoration:none;">Learning Video Representations using Contrastive Bidirectional Transformer</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(163).pdf" style="text-decoration:none;">Language as an Abstraction
for Hierarchical Deep Reinforcement Learning </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(164).pdf" style="text-decoration:none;">Distilling Translations with Visual Awareness</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(165).pdf" style="text-decoration:none;">RUBi: Reducing Unimodal Biases
for Visual Question Answering</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(166).pdf" style="text-decoration:none;">Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(167).pdf" style="text-decoration:none;">Language2Pose: Natural Language Grounded Pose Forecasting</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(168).pdf" style="text-decoration:none;">Vision-and-Dialog Navigation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(169).pdf" style="text-decoration:none;">OmniNet: A unified architecture for multi-modal multi-task learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(170).pdf" style="text-decoration:none;">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(171).pdf" style="text-decoration:none;">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(172).pdf" style="text-decoration:none;">VideoNavQA: Bridging the Gap between
Visual and Embodied Question Answering</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(173).pdf" style="text-decoration:none;">Fusion of Detected Objects in Text for Visual Question Answering</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(174).pdf" style="text-decoration:none;">Integrating Multimodal Information in Large Pretrained Transformers</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(175).pdf" style="text-decoration:none;">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(176).pdf" style="text-decoration:none;">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(177).pdf" style="text-decoration:none;">ViCo: Word Embeddings from Visual Co-occurrences</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(178).pdf" style="text-decoration:none;">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(179).pdf" style="text-decoration:none;">Towards Unsupervised Image Captioning with Shared Multimodal Embeddings</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(180).pdf" style="text-decoration:none;">Interactive Language Learning by Question Answering</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(181).pdf" style="text-decoration:none;">RTFM: Generalising to Novel Environment Dynamics via Reading</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(182).pdf" style="text-decoration:none;">Heterogeneous Graph Learning for Visual
Commonsense Reasoning </a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(183).pdf" style="text-decoration:none;">Few-shot Video-to-Video Synthesis</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(184).pdf" style="text-decoration:none;">Shaping Visual Representations with Language for Few-Shot Classification</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(185).pdf" style="text-decoration:none;">Dynamic Fusion for Multimodal Data</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(186).pdf" style="text-decoration:none;">Affective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey</a></li>
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(187).pdf" style="text-decoration:none;">Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</a></li>
                             
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(188).pdf" style="text-decoration:none;">Two Causal Principles for Improving Visual Dialog</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(189).pdf" style="text-decoration:none;">Self-Supervised Learning by Cross-Modal Audio-Video Clustering</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(190).pdf" style="text-decoration:none;">12-in-1: Multi-Task Vision and Language Representation Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(191).pdf" style="text-decoration:none;">End-to-end facial and physiological model for Affective Computing and applications  </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(192).pdf" style="text-decoration:none;">A Logical Model for Supporting Social Commonsense Knowledge Acquisition</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(193).pdf" style="text-decoration:none;">Visual Agreement Regularized Training for Multi-Modal Machine Translation</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(194).pdf" style="text-decoration:none;">Towards Learning a Generic Agent for
Vision-and-Language Navigation via Pre-training</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(195).pdf" style="text-decoration:none;">Visual Grounding in Video for UnsupervisedWord Translation</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(196).pdf" style="text-decoration:none;">VIOLIN: A Large-Scale Dataset for Video-and-Language Inference</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(197).pdf" style="text-decoration:none;">Music Gesture for Visual Sound Separation</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(198).pdf" style="text-decoration:none;">Improving Vision-and-Language Navigation with Image-Text Pairs from the Web</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(199).pdf" style="text-decoration:none;">Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(200).pdf" style="text-decoration:none;">The Hateful Memes Challenge:
Detecting Hate Speech in Multimodal Memes</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(201).pdf" style="text-decoration:none;">Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(202).pdf" style="text-decoration:none;">CoMIR: Contrastive Multimodal Image
Representation for Registration</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(203).pdf" style="text-decoration:none;">Labelling unlabelled videos
from scratch with multi-modal self-supervision</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(204).pdf" style="text-decoration:none;">Self-Supervised MultiModal Versatile Networks</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(205).pdf" style="text-decoration:none;">Towards Debiasing Sentence Representations</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(206).pdf" style="text-decoration:none;">
Grounded Language Learning Fast and Slow</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(207).pdf" style="text-decoration:none;">FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(208).pdf" style="text-decoration:none;">Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(209).pdf" style="text-decoration:none;">Strategies for Multi-Modal Scene Exploration</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(210).pdf" style="text-decoration:none;">Deep Multimodal Fusion by Channel Exchanging</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(211).pdf" style="text-decoration:none;">Multimodal Transformer for Multimodal Machine Translation</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(212).pdf" style="text-decoration:none;">What Does BERT with Vision Look At?</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(213).pdf" style="text-decoration:none;">Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(214).pdf" style="text-decoration:none;">Simplifying Coreference Chains for Dyslexic Children</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(215).pdf" style="text-decoration:none;">Multimodal Learning with Deep Boltzmann Machines</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(216).pdf" style="text-decoration:none;">Deep Canonical Correlation Analysis</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(217).pdf" style="text-decoration:none;">Look, Listen and Learn</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(218).pdf" style="text-decoration:none;">Machine Learning in Multimodal Medical Imaging</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(219).pdf" style="text-decoration:none;">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(220).pdf" style="text-decoration:none;">IEMOCAP: Interactive emotional dyadic motion capture database</a></li>
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(221).pdf" style="text-decoration:none;">Is an ImageWorth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and Linguistic Representations</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(222).pdf" style="text-decoration:none;">An empirical study on the effectiveness of images in Multimodal Neural Machine Translation</a></li> 


<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(223).pdf" style="text-decoration:none;">Incorporating Global Visual Features into Attention-Based Neural Machine Translation</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(224).pdf" style="text-decoration:none;">TVQA: Localized, Compositional Video Question Answering</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(225).pdf" style="text-decoration:none;">Adversarial Evaluation of Multimodal Machine Translation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(226).pdf" style="text-decoration:none;">A Visual Attention Grounding Neural Model for Multimodal Machine Translation</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(227).pdf" style="text-decoration:none;">Collecting Large, Richly Annotated
Facial-Expression Databases from Movies</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(228).pdf" style="text-decoration:none;">See, feel, act: Hierarchical learning for complex manipulation skills with multisensory fusion</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(229).pdf" style="text-decoration:none;">Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(230).pdf" style="text-decoration:none;"> Emergent Communication in a Multi-Modal, Multi-Step Referential Game </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(231).pdf" style="text-decoration:none;">
Emergent Communication through Negotiation</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(232).pdf" style="text-decoration:none;">Emergent Translation in Multi-Agent Communication</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(233).pdf" style="text-decoration:none;">Stacked Latent Attention for Multimodal Reasoning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(234).pdf" style="text-decoration:none;">Adventures in Flatland: Perceiving Social Interactions Under Physical Dynamics</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(235).pdf" style="text-decoration:none;">Multi-Modal Scene Understanding
for Robotic Grasping</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(236).pdf" style="text-decoration:none;">Learning to Separate Object Sounds by Watching Unlabeled Video</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(237).pdf" style="text-decoration:none;">Generative Pretraining from Pixels</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(238).pdf" style="text-decoration:none;">Finding "It": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(239).pdf" style="text-decoration:none;">Multi-modal Predicate Identification
using Dynamically Learned Robot Controllers</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(240).pdf" style="text-decoration:none;">What does BERT learn about the structure of language?</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(241).pdf" style="text-decoration:none;">Multimodal Neural Language Models</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(242).pdf" style="text-decoration:none;">Language Models are Unsupervised Multitask Learners</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(243).pdf" style="text-decoration:none;">The Multi-Entity Variational Autoencoder</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(244).pdf" style="text-decoration:none;">Mapping Instructions and Visual Observations to Actions with Reinforcement Learning</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(245).pdf" style="text-decoration:none;">Decoding Children's Social Behavior</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(246).pdf" style="text-decoration:none;">Combining Language and Vision
with a Multimodal Skip-gram Model</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(247).pdf" style="text-decoration:none;">CLEVR-Dialog:
A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(248).pdf" style="text-decoration:none;">Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(249).pdf" style="text-decoration:none;">Probing the Need for Visual Context in Multimodal Machine Translation</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(250).pdf" style="text-decoration:none;">Neural Machine Translation with Universal Visual Representation</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(251).pdf" style="text-decoration:none;">Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(252).pdf" style="text-decoration:none;">Visual Concept-Metaconcept Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(253).pdf" style="text-decoration:none;">DeViSE: A Deep Visual-Semantic Embedding Model</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(254).pdf" style="text-decoration:none;">Learning Multiagent Communication
with Backpropagation</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(255).pdf" style="text-decoration:none;">Unsupervised Learning of Spoken Language with Visual Context</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(256).pdf" style="text-decoration:none;">Grounded Language Learning from Video Described with Sentences</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(257).pdf" style="text-decoration:none;">Learning Grounded Meaning Representations with Autoencoders</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(258).pdf" style="text-decoration:none;">Multimodal Pivots for Image Caption Translation</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(259).pdf" style="text-decoration:none;">Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(260).pdf" style="text-decoration:none;">Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(261).pdf" style="text-decoration:none;">Learning Translations via Images with a Massively Multilingual Image Dataset</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(40).pdf" style="text-decoration:none;">Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(41).pdf" style="text-decoration:none;">SCAN: Learning Hierarchical Compositional Visual Concepts</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(42).pdf" style="text-decoration:none;">Tensor Fusion Network for Multimodal Sentiment Analysis</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(43).pdf" style="text-decoration:none;">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(44).pdf" style="text-decoration:none;">Localizing Moments in Video with Natural Language</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(45).pdf" style="text-decoration:none;">Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(46).pdf" style="text-decoration:none;">Cross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(47).pdf" style="text-decoration:none;">Fooling Vision and Language Models
Despite Localization and Attention Mechanism</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(48).pdf" style="text-decoration:none;">Emergent Translation in Multi-Agent Communication</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(49).pdf" style="text-decoration:none;">Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(50).pdf" style="text-decoration:none;">Multimodal Probabilistic Model-Based Planning for Human-Robot Interaction</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(51).pdf" style="text-decoration:none;">Learning Multi-ModalWord Representation Grounded in Visual Context</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(52).pdf" style="text-decoration:none;">Look, Imagine and Match:
Improving Textual-Visual Cross-Modal Retrieval with Generative Models</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(53).pdf" style="text-decoration:none;">Neural Motifs: Scene Graph Parsing with Global Context</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(54).pdf" style="text-decoration:none;">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(55).pdf" style="text-decoration:none;">Video Captioning via Hierarchical Reinforcement Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(56).pdf" style="text-decoration:none;">Embodied Question Answering </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(57).pdf" style="text-decoration:none;">Don't Just Assume; Look and Answer:
Overcoming Priors for Visual Question Answering</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(58).pdf" style="text-decoration:none;">Grounding Referring Expressions in Images by Variational Context</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Multimodal-ML-Papers/blob/master/cv(59).pdf" style="text-decoration:none;">Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning</a></li>
 





 
 </ul>


